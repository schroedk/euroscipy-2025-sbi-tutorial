{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Your First Inference with SBI\n",
    "\n",
    "**Time:** 15 minutes  \n",
    "**Goal:** Run neural posterior estimation on the Lotka-Volterra predator-prey model\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this exercise, you will:\n",
    "1. Set up a simulator for SBI\n",
    "2. Define a prior over parameters\n",
    "3. Train a neural posterior estimator\n",
    "4. Visualize and interpret the posterior distribution\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ The Story\n",
    "\n",
    "You're a data scientist for a regional environmental agency. The farmers and hunters\n",
    "south of Krakow have collected a dataset of daily counts of wolves and deers. However,\n",
    "due to data privacy issues, the data team cannot give these daily counts to you\n",
    "directly. Instead, you receive summary statistics of the data: for now only the mean,\n",
    "std, max, kurtosis and skew for each population over a span of 200 days.\n",
    "\n",
    "These summary statistics are your only window into the ecosystem's health.\n",
    "\n",
    "**Your mission:** Use these sparse summary statistics to infer the continuous-time\n",
    "dynamics of the predator-prey relationship and predict how the populations might evolve\n",
    "in the future.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import and Setup\n",
    "\n",
    "First, let's import everything we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "from sbi import utils as utils\n",
    "\n",
    "# SBI imports\n",
    "from sbi.inference import NPE, simulate_for_sbi\n",
    "\n",
    "# Tutorial-specific imports\n",
    "from simulators.lotka_volterra import (\n",
    "    create_lotka_volterra_prior,\n",
    "    generate_observed_data,\n",
    "    get_summary_labels,\n",
    "    lotka_volterra_simulator,\n",
    "    simulate,\n",
    ")\n",
    "from utils import (\n",
    "    generate_posterior_predictive_simulations,\n",
    "    plot_posterior_predictions,\n",
    "    print_summary_statistics,\n",
    "    analyze_posterior_statistics,\n",
    ")\n",
    "\n",
    "# Set style for nice plots\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "plt.rcParams[\"axes.labelsize\"] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42  # For reproducibility\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# üîß CONFIGURATION: Toggle this to see the impact of different summary statistics\n",
    "USE_AUTOCORRELATION = False  # Set to True to include temporal dynamics\n",
    "\n",
    "# Simulation settings\n",
    "num_simulations = 20_000\n",
    "# TODO: Adjust number of workers based on your system.\n",
    "num_workers = 10\n",
    "\n",
    "print(\"‚úÖ Simulator ready! Let's analyze deer-wolf dynamics in southern Poland.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load the Observed Data\n",
    "\n",
    "Let's load and visualize the population statistics that your field team collected from the forests south of Krak√≥w:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the observed data for our scenario.\n",
    "observed_data, true_params = generate_observed_data(\n",
    "    use_autocorrelation=USE_AUTOCORRELATION\n",
    ")\n",
    "# Note that true_params would not be available in a real scenario.\n",
    "\n",
    "# Create a configured simulator that uses our chosen autocorrelation setting\n",
    "lotka_volterra_simulator = partial(\n",
    "    lotka_volterra_simulator, use_autocorrelation=USE_AUTOCORRELATION\n",
    ")\n",
    "\n",
    "# Get labels and print statistics using utility function\n",
    "data_labels = get_summary_labels(use_autocorrelation=USE_AUTOCORRELATION)\n",
    "print_summary_statistics(observed_data, data_labels, USE_AUTOCORRELATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define the Prior\n",
    "\n",
    "Before we can infer parameters, we need to specify our prior beliefs about reasonable parameter ranges.\n",
    "\n",
    "The Lotka-Volterra model has 4 parameters:\n",
    "- **Œ±**: Deer birth rate (how fast deer reproduce)\n",
    "- **Œ≤**: Predation rate (how efficiently wolves hunt deer)  \n",
    "- **Œ¥**: Wolf efficiency (how well wolves convert deer to offspring)\n",
    "- **Œ≥**: Wolf death rate (natural wolf mortality)\n",
    "\n",
    "Based on ecological knowledge from Polish forests, we set uniform priors over plausible ranges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use our expert domain knowledge to set uniform prior boundaries.\n",
    "prior = create_lotka_volterra_prior()\n",
    "\n",
    "# Sample a few parameter sets to see the prior\n",
    "print(\"\\nüé≤ Example parameter samples from prior:\")\n",
    "prior_samples = prior.sample((3,))\n",
    "for i, sample in enumerate(prior_samples):\n",
    "    print(\n",
    "        f\"  Sample {i + 1}: Œ±={sample[0]:.2f}, Œ≤={sample[1]:.3f}, Œ¥={sample[2]:.3f}, Œ≥={sample[3]:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: The Magic - Neural Posterior Estimation in 5 Lines! ‚ú®\n",
    "\n",
    "Now comes the exciting part. With just a few lines of code, we'll:\n",
    "1. Create a neural posterior estimator\n",
    "2. Generate training simulations\n",
    "3. Train the neural network\n",
    "4. Get the posterior distribution\n",
    "\n",
    "Watch how simple this is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE 5 LINES OF SBI! üöÄ\n",
    "\n",
    "# Line 1: Create the inference object\n",
    "npe = NPE(prior)\n",
    "\n",
    "# Line 2: Generate training data (this might take ~30 seconds)\n",
    "print(\"üîÑ Generating training simulations...\")\n",
    "theta, x = simulate_for_sbi(\n",
    "    lotka_volterra_simulator,\n",
    "    prior,\n",
    "    num_simulations=num_simulations,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "print(f\"‚úÖ Generated {len(theta)} simulations\")\n",
    "\n",
    "# Line 3: Train the neural network (this might take ~1 minute)\n",
    "print(\"\\nüß† Training neural posterior estimator...\")\n",
    "npe.append_simulations(theta, x).train()\n",
    "print(\"‚úÖ Training complete!\")\n",
    "\n",
    "# Line 4: Build posterior for our observed data\n",
    "posterior = npe.build_posterior()\n",
    "\n",
    "# Line 5: Sample from the posterior\n",
    "print(\"\\nüìà Sampling from posterior...\")\n",
    "posterior_samples: torch.Tensor = posterior.sample((10000,), x=observed_data)  # type: ignore\n",
    "print(f\"‚úÖ Drew {len(posterior_samples)} posterior samples\")\n",
    "\n",
    "print(\"\\nüéâ Inference complete! Let's see what we learned...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the posterior for later.\n",
    "filename = f\"lv_inference{'_autocorr' if USE_AUTOCORRELATION else ''}.pt\"\n",
    "with open(filename, \"wb\") as handle:\n",
    "    pickle.dump(npe, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize the Results\n",
    "\n",
    "Now let's see what the neural network learned about the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the posterior distributions\n",
    "from sbi.analysis import pairplot\n",
    "\n",
    "fig = pairplot(\n",
    "    posterior_samples,\n",
    "    points=true_params,\n",
    "    labels=[r\"$\\alpha$\", r\"$\\beta$\", r\"$\\delta$\", r\"$\\gamma$\"],\n",
    "    figsize=(10, 8),\n",
    "    points_colors=\"red\",\n",
    "    points_kwargs={\"s\": 100, \"marker\": \"*\", \"label\": \"True parameters\"},\n",
    "    labels_params={\"fontsize\": 5},\n",
    "    ticks_params={\"labelsize\": 14},\n",
    "    # use prior limits\n",
    "    limits=[\n",
    "        (low, high)\n",
    "        for (low, high) in zip(prior.base_dist.low, prior.base_dist.high, strict=False)\n",
    "    ],\n",
    ")\n",
    "\n",
    "plt.suptitle(\"Posterior Distribution of Lotka-Volterra Parameters\", fontsize=20, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Interpret the Results\n",
    "\n",
    "Let's extract key statistics from our posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter names for our Lotka-Volterra model\n",
    "param_names = [\n",
    "    \"Œ± (deer birth)\",\n",
    "    \"Œ≤ (predation)\",\n",
    "    \"Œ¥ (wolf efficiency)\",\n",
    "    \"Œ≥ (wolf death)\",\n",
    "]\n",
    "\n",
    "# Use our minimal function to analyze posterior statistics\n",
    "posterior_stats = analyze_posterior_statistics(\n",
    "    posterior_samples=posterior_samples,\n",
    "    param_names=param_names,\n",
    "    true_params=true_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Key Insights About Posterior Correlations\n",
    "\n",
    "The above 1D and 2D marginals of the posterior distribution are relatively broad,\n",
    "indicating that we can observe same data patterns across different parameters. This\n",
    "happens because:\n",
    "\n",
    "1. **Parameter Compensation**: Changing one parameter can be compensated by adjusting\n",
    "   another, and vice versa\n",
    "2. **Identifiability Challenges**: Multiple parameter combinations can produce similar\n",
    "   summary statistics\n",
    "3. **Statistical Grounding**: These correlations are statistically principled, which\n",
    "   would be difficult to achieve with optimization alone\n",
    "\n",
    "Understanding these correlations helps us:\n",
    "- Identify which parameters are most constrained by the data\n",
    "- Recognize when additional data or different summary statistics might be needed\n",
    "- Interpret the biological meaning of parameter trade-offs in the ecosystem model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Predict Future Population Dynamics\n",
    "\n",
    "Now for the most impactful part of our analysis. We have inferred the hidden parameters\n",
    "of the ecosystem from just a few summary numbers. Can we use this to predict how the\n",
    "deer and wolf populations will evolve over the weeks?\n",
    "\n",
    "Yes! We can take parameter sets from our posterior, run the full simulation forward in\n",
    "time for each of them, and visualize the results. This will give us a **distribution of\n",
    "possible futures**, complete with uncertainty bands.\n",
    "\n",
    "The plotting function below will draw 1000 posterior samples, calculate the median\n",
    "samples and the maximum of the posterior (Maximum a Posterior estimate, MAP), and\n",
    "simulate all those parameters with the LV simulator without calculating summary\n",
    "statistics (using the entire time series over 200 days). This shows our prediction of\n",
    "how the wolve and deer populations will evolve and it will inform political decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate posterior predictive simulations and MAP simulation\n",
    "map_simulation, predictive_simulations = generate_posterior_predictive_simulations(\n",
    "    posterior=posterior,\n",
    "    observed_data=observed_data,\n",
    "    simulate_func=simulate,\n",
    "    prior=prior,\n",
    "    num_simulations=1000,\n",
    "    num_workers=10,\n",
    ")\n",
    "\n",
    "# Plot the results using our utility function\n",
    "fig, ax = plot_posterior_predictions(\n",
    "    predictions=predictive_simulations,\n",
    "    map_prediction=map_simulation,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Optional Exercise: Improving Predictions with Better Statistics\n",
    "\n",
    "**Scenario:** After presenting your results, the farmers and hunters were concerned about the high uncertainty in your predictions, especially how quickly the confidence intervals grow after just a few weeks. They need more reliable forecasts for planning their wildlife management strategies.\n",
    "\n",
    "You explain that better predictions require more informative data. After some negotiation, the data team agrees to provide additional temporal information: **5 autocorrelation lags** for each population, capturing how current population levels relate to past levels.\n",
    "\n",
    "**Your Task:** \n",
    "1. Set `USE_AUTOCORRELATION = True` in the configuration cell above\n",
    "2. Re-run the analysis from Step 2 onwards\n",
    "3. Compare the new results with your previous analysis\n",
    "\n",
    "**Questions to explore:**\n",
    "- How do the posterior distributions change with more informative statistics?\n",
    "- How does the prediction uncertainty compare between the two approaches?\n",
    "- What does this tell us about the value of temporal information in ecological modeling?\n",
    "\n",
    "**Hint:** You'll need to restart from the data generation step since the summary statistics will be different!\n",
    "\n",
    "**Expected outcome:** You should see much tighter posterior distributions and significantly reduced uncertainty in long-term predictions, demonstrating the power of informative summary statistics in SBI!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "Congratulations! You've just completed your first SBI analysis. Here's what you accomplished:\n",
    "\n",
    "1.  **Inference from summaries**: We inferred the parameters of a complex time-series model using only a few summary statistics, mimicking a real-world scenario of sparse data.\n",
    "2.  **Full time-series prediction**: We used the posterior to generate not just summary predictions, but full, high-resolution forecasts of future population dynamics.\n",
    "3.  **Powerful uncertainty quantification**: We didn't just get a single future trajectory; we generated a distribution of possible futures, complete with credible intervals that show how uncertainty grows over time.\n",
    "4.  **Bridging data gaps**: This workflow shows how to move from sparse, aggregated data (monthly reports) to rich, actionable insights (continuous population forecasts).\n",
    "\n",
    "### üå≤ Applications to Environmental Management\n",
    "\n",
    "This kind of analysis is incredibly valuable for decision-making:\n",
    "-   **Proactive Policy**: Instead of reacting to population crashes, the agency can see potential futures and act preemptively.\n",
    "-   **Risk Assessment**: The width of the credible intervals quantifies the range of plausible outcomes, helping to assess best-case and worst-case scenarios.\n",
    "-   **Resource Allocation**: Understanding the likely trajectories helps in planning conservation efforts, such as habitat restoration or intervention measures.\n",
    "\n",
    "### ‚ö†Ô∏è Important Questions\n",
    "\n",
    "Before we trust these results for forest management decisions, we should ask:\n",
    "1. Is the neural network approximation accurate?\n",
    "2. Would different summary statistics give us better inference?\n",
    "3. How many simulations do we really need?\n",
    "4. Are we losing important information by summarizing?\n",
    "\n",
    "**‚Üí These questions are addressed in Exercise 2: Diagnostics!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space for experimentation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "euroscipy-2025-sbi-tutorial (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
