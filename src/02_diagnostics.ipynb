{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Diagnostic Checks for SBI üîç\n",
    "\n",
    "**Time:** 20 minutes  \n",
    "**Difficulty:** Intermediate  \n",
    "\n",
    "In this exercise, you'll learn how to validate your neural posterior estimator using diagnostic tools. These checks are crucial for building trust in your inference results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this exercise, you will be able to:\n",
    "\n",
    "1. ‚úÖ Perform prior predictive checks to validate your prior\n",
    "2. ‚úÖ Monitor neural network training with loss curves\n",
    "3. ‚úÖ Run posterior predictive checks to validate inference\n",
    "4. ‚úÖ Use simulation-based calibration (SBC) to test the inference method\n",
    "5. ‚úÖ Interpret diagnostic results and identify problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ The Story Continues...\n",
    "\n",
    "Your initial analysis of the wolf-deer populations impressed the environmental agency! However, before they implement costly interventions based on your predictions, the senior ecologist asks:\n",
    "\n",
    "*\"How do we know the neural network learned correctly? What if it's just memorizing patterns rather than understanding the true dynamics? These predictions will inform important decisions about hunting quotas and conservation efforts.\"*\n",
    "\n",
    "Excellent question! Let's verify our results with diagnostic tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Quick Inference Recap\n",
    "\n",
    "Let's quickly repeat the inference from Exercise 1 (with fewer simulations for speed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sbi import utils as utils\n",
    "from sbi.diagnostics import run_sbc\n",
    "from sbi.inference import simulate_for_sbi\n",
    "\n",
    "# Our simulator\n",
    "from simulators.lotka_volterra import (\n",
    "    create_lotka_volterra_prior,\n",
    "    generate_observed_data,\n",
    "    get_lv_summary_stats_names,\n",
    "    lotka_volterra_simulator,\n",
    ")\n",
    "\n",
    "# Import our utility functions\n",
    "from utils import plot_predictive_check, plot_training_diagnostics\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "plt.rcParams[\"axes.labelsize\"] = 16\n",
    "\n",
    "# Setup\n",
    "USE_AUTOCORRELATION = True\n",
    "prior = create_lotka_volterra_prior()\n",
    "observed_data, true_params = generate_observed_data(\n",
    "    use_autocorrelation=USE_AUTOCORRELATION\n",
    ")\n",
    "lotka_volterra_simulator = partial(\n",
    "    lotka_volterra_simulator, use_autocorrelation=USE_AUTOCORRELATION\n",
    ")\n",
    "num_workers: int = 5\n",
    "\n",
    "# Let's load the inference object from exercise 01\n",
    "with open(\n",
    "    f\"lv_inference_{'with_autocorrelation' if USE_AUTOCORRELATION else 'without_autocorrelation'}.pt\",\n",
    "    \"rb\",\n",
    ") as handle:\n",
    "    npe = pickle.load(handle)\n",
    "# And build a new posterior object\n",
    "posterior = npe.build_posterior(prior=prior)\n",
    "# Set default x to observed data\n",
    "posterior.set_default_x(observed_data)\n",
    "\n",
    "print(\"\\n‚úÖ Inference loaded! Now let's check if we can trust it...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prior Predictive Check\n",
    "\n",
    "**Key Question:** *Does our observed data fall within the range of data that the prior can generate?*\n",
    "\n",
    "Before looking at the posterior, we should check if our prior is reasonable:\n",
    "1. Sample parameters from the prior\n",
    "2. Simulate data with those parameters  \n",
    "3. Check if observed data falls within this distribution\n",
    "\n",
    "If the observed data is far outside the prior predictive distribution, we may need to reconsider our prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Diagnostic 1: Prior Predictive Check\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Checking if our observed data is consistent with the prior...\\n\")\n",
    "\n",
    "# Define the statistics names for the Lotka-Volterra model\n",
    "stat_names = get_lv_summary_stats_names(USE_AUTOCORRELATION)\n",
    "\n",
    "# Simulate data from the prior\n",
    "# theta, x = simulate_for_sbi(lotka_volterra_simulator, prior, num_simulations=10000, num_workers=num_workers)\n",
    "x = npe._x_roundwise[0]\n",
    "prior_data_limits: list[tuple[float, float]] = [\n",
    "    (x.min(dim=0).values[i], x.max(dim=0).values[i]) for i in range(x.shape[1])\n",
    "]\n",
    "\n",
    "# Run the prior predictive check using our reusable function\n",
    "plot_predictive_check(\n",
    "    x=x,\n",
    "    observed_data=observed_data,\n",
    "    stat_names=stat_names,\n",
    "    title=\"Prior Predictive Check: Is our observed data consistent with the prior?\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Neural Network Training Diagnostics\n",
    "\n",
    "**Key Question:** *Did the neural network converge during training?*\n",
    "\n",
    "We need to check if the neural density estimator was trained properly:\n",
    "- Training loss should decrease and stabilize\n",
    "- Validation loss should not increase (no overfitting)\n",
    "- Both loss curves should ideally converge to a plateaux\n",
    "\n",
    "The `NPE` trainer class saves training and validation loss as well as other statistics\n",
    "during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can just pass the npe object to our plotting function.\n",
    "plot_training_diagnostics(npe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Posterior Predictive Check\n",
    "\n",
    "**Key Question:** *If we simulate data using parameters from our posterior, does it look like our observed data?*\n",
    "\n",
    "This is the most intuitive diagnostic:\n",
    "1. Sample parameters from the posterior\n",
    "2. Simulate data with those parameters\n",
    "3. Compare simulated data to observations\n",
    "\n",
    "If the posterior is correct, simulated data should be consistent with observations,\n",
    "e.g., it should \"center around\" the observed data. Some fluctuations around the observed\n",
    "data is valid, e.g., this could be due to simulator noise and parameter uncertainty.\n",
    "However, when the observed data lies at the edges or outside of the posterior predictive\n",
    "distributions, this hints at a problem with the overall inference setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior predictive checks: simulate data with parameters samples from the posterior.\n",
    "_, predictive_sims = simulate_for_sbi(\n",
    "    lotka_volterra_simulator, posterior, num_simulations=10000, num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the posterior predictive distributions together with the observed data.\n",
    "plot_predictive_check(\n",
    "    predictive_sims,\n",
    "    observed_data,\n",
    "    stat_names=stat_names,\n",
    "    limits=prior_data_limits,\n",
    "    title=\"Posterior Predictive Check: Are our posterior samples consistent with the observed data?\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Simulation-Based Calibration (SBC)\n",
    "\n",
    "**Key Question:** *When we say \"90% confident\", are we right 90% of the time?*\n",
    "\n",
    "Before the environmental agency commits millions to conservation efforts, they need to\n",
    "know: **Can we trust our uncertainty estimates?**\n",
    "\n",
    "### What is SBC?\n",
    "Simulation-Based Calibration tests whether our Bayesian credible intervals are\n",
    "statistically reliable:\n",
    "\n",
    "- **The Test**: We pretend to be nature 200+ times:\n",
    "  1. Sample a \"true\" parameters from our prior\n",
    "  2. Simulate fake observations with these parameters  \n",
    "  3. Run inference to get a posterior\n",
    "  4. Check: Does the posterior contain the truth?\n",
    "  \n",
    "- **Expected Result**: If our method works correctly:\n",
    "  - 50% credible intervals should contain the truth ~50% of the time\n",
    "  - 90% credible intervals should contain the truth ~90% of the time\n",
    "  - The \"rank\" of true parameters should be uniformly distributed\n",
    "\n",
    "### What Can Go Wrong?\n",
    "SBC reveals different failure modes through the shape of the rank distribution:\n",
    "- **‚à©-shaped** (underconfident): Posteriors too wide, like always saying \"I don't know\"\n",
    "- **‚à™-shaped** (overconfident): Posteriors too narrow, false precision\n",
    "- **Skewed**: Systematic bias in one direction\n",
    "\n",
    "‚ö†Ô∏è **Important**: A perfect SBC doesn't mean perfect inference! You could pass SBC by\n",
    "always returning the prior (maximally uncertain). That's why we combine SBC with\n",
    "posterior predictive checks‚Äîto ensure we're both calibrated AND informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Diagnostic 5: Simulation-Based Calibration (SBC)\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nüìä The Ultimate Trust Test:\")\n",
    "print(\"   If we claim 90% confidence, are we right 90% of the time?\")\n",
    "print(\"\\nüî¨ Method: Running 200 'fake experiments' where we know the truth\")\n",
    "print(\"   For each: simulate data ‚Üí run inference ‚Üí check if truth is recovered\")\n",
    "print(\"\\n‚è±Ô∏è  This may take a minute... (in practice, use 500+ tests)\\n\")\n",
    "\n",
    "# Run SBC using sbi's built-in function\n",
    "num_sbc_runs = 200\n",
    "num_posterior_samples = 1000\n",
    "\n",
    "thetas, xs = simulate_for_sbi(\n",
    "    lotka_volterra_simulator,\n",
    "    prior,\n",
    "    num_simulations=num_sbc_runs,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "print(f\"Running {num_sbc_runs} SBC tests...\")\n",
    "ranks, dap_samples = run_sbc(\n",
    "    thetas,\n",
    "    xs,\n",
    "    posterior,\n",
    "    num_posterior_samples=num_posterior_samples,\n",
    "    reduce_fns=\"marginals\",\n",
    "    use_batched_sampling=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.analysis import sbc_rank_plot\n",
    "from sbi.diagnostics import check_sbc\n",
    "\n",
    "# Perform statistical checks on the SBC results.\n",
    "ks_pvals, c2st_ranks, c2st_daps = check_sbc(ranks, thetas, dap_samples).values()\n",
    "\n",
    "param_names = [\n",
    "    \"Œ± (deer birth)\",\n",
    "    \"Œ≤ (predation)\",\n",
    "    \"Œ¥ (wolf efficiency)\",\n",
    "    \"Œ≥ (wolf death)\",\n",
    "]\n",
    "\n",
    "print(\"üìà Visualizing SBC Results: the empirical CDFs should lie inside the grey area.\")\n",
    "fig, ax = sbc_rank_plot(\n",
    "    ranks,\n",
    "    num_posterior_samples=num_posterior_samples,\n",
    "    num_bins=100,\n",
    "    parameter_labels=param_names,\n",
    "    plot_type=\"hist\",\n",
    "    ranks_labels=[\"Lotka-Volterra\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìà SBC Results per Parameter:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "param_names_short = [\"Œ±\", \"Œ≤\", \"Œ¥\", \"Œ≥\"]\n",
    "\n",
    "# Check each dimension\n",
    "all_pass_ks = True\n",
    "all_pass_c2st = True\n",
    "\n",
    "for i, param in enumerate(param_names_short):\n",
    "    ks_p = ks_pvals[i].item()\n",
    "    c2st_r = c2st_ranks[i].item()\n",
    "\n",
    "    ks_ok = ks_p > 0.05\n",
    "    c2st_ok = 0.4 < c2st_r < 0.6\n",
    "\n",
    "    all_pass_ks = all_pass_ks and ks_ok\n",
    "    all_pass_c2st = all_pass_c2st and c2st_ok\n",
    "\n",
    "    status = \"‚úÖ\" if (ks_ok and c2st_ok) else \"‚ö†Ô∏è\"\n",
    "    print(f\"{status} {param}: KS p={ks_p:.3f}, C2ST={c2st_r:.3f}\")\n",
    "\n",
    "# Brief interpretation\n",
    "print(\"\\nüéØ Bottom Line:\")\n",
    "if all_pass_ks and all_pass_c2st:\n",
    "    print(\"   ‚úÖ All parameters well-calibrated!\")\n",
    "    print(\"   ‚Üí Credible intervals are trustworthy\")\n",
    "else:\n",
    "    flagged = [\n",
    "        param_names_short[i]\n",
    "        for i in range(4)\n",
    "        if ks_pvals[i] <= 0.05 or c2st_ranks[i] >= 0.6\n",
    "    ]\n",
    "    print(f\"   ‚ö†Ô∏è  Calibration issues in: {', '.join(flagged)}\")\n",
    "    print(\"   ‚Üí Review these parameters before decisions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "### Why Diagnostics Matter\n",
    "\n",
    "1. **Neural networks can fail silently** - They might produce confident-looking but wrong results\n",
    "2. **Not all posteriors are created equal** - Some might be overconfident, others too uncertain\n",
    "3. **Trust but verify** - Always check your inference before making decisions\n",
    "4. **Diagnostics catch different failures** - Each test reveals specific problems\n",
    "\n",
    "### Your Diagnostic Toolkit\n",
    "\n",
    "| Diagnostic | What it checks | Red flag | What it catches |\n",
    "|------------|---------------|----------|-----------------|\n",
    "| Prior Predictive | Prior covers observations | Observations in extreme tails | Bad prior specification |\n",
    "| Training Diagnostics | Network convergence | Loss increasing or unstable | Insufficient training |\n",
    "| Posterior Predictive | Can recreate observations | Observations outside CI | Too little or uninformative data |\n",
    "| SBC | Calibrated credible intervals | Non-uniform rank histograms | Over/underconfident posteriors |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "‚úÖ **Always run diagnostics** - Make it part of your workflow  \n",
    "‚úÖ **Start with the prior** - Bad prior ‚Üí bad posterior  \n",
    "‚úÖ **Document results** - Keep diagnostic reports with your analysis  \n",
    "‚úÖ **Iterate if needed** - Poor diagnostics ‚Üí adjust and retrain  \n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Challenge: Stress Testing\n",
    "\n",
    "Try breaking the inference and seeing how diagnostics detect it:\n",
    "\n",
    "1. **Too few simulations**: Retrain with only 500 simulations\n",
    "2. **Wrong prior**: Use a prior that doesn't contain true parameters\n",
    "3. **Corrupted data**: Add extreme outliers to observations\n",
    "\n",
    "How do the diagnostics change? Which tests catch which problems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space for experimentation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "euroscipy-2025-sbi-tutorial (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
