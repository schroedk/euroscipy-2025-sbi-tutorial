{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise 3: Apply SBI to Your Own Problem üöÄ\n",
        "\n",
        "**Time:** 20 minutes  \n",
        "**Goal:** Apply what you've learned to a new simulator\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "By the end of this exercise, you will:\n",
        "1. ‚úÖ Adapt the SBI workflow to a new problem\n",
        "2. ‚úÖ Define appropriate priors for your parameters\n",
        "3. ‚úÖ Run inference and diagnostics on your simulator\n",
        "4. ‚úÖ Leave with working code you can adapt\n",
        "\n",
        "## Choose Your Adventure!\n",
        "\n",
        "We provide two well-tested example simulators, or you can bring your own:\n",
        "\n",
        "### üéæ Option A: Ball Throw Physics\n",
        "- **Story**: You're analyzing baseball pitches or golf drives\n",
        "- **Physics**: Projectile motion with air resistance\n",
        "- **Challenge**: Infer launch conditions from landing position\n",
        "\n",
        "### ü¶† Option B: SIR Epidemic Model\n",
        "- **Story**: You're tracking disease spread in a community\n",
        "- **Model**: Classic compartmental epidemic model\n",
        "- **Challenge**: Infer transmission rates from outbreak data\n",
        "\n",
        "### üî¨ Option C: Your Own Simulator\n",
        "- Bring your research problem!\n",
        "- We'll help you adapt it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# SBI imports\n",
        "from sbi import inference\n",
        "from sbi import analysis\n",
        "from sbi import utils\n",
        "\n",
        "# Our example simulators\n",
        "import sys\n",
        "\n",
        "sys.path.append(\"..\")\n",
        "from simulators.ball_throw import ball_throw_simulator, create_ball_throw_prior\n",
        "from simulators.sir_model import sir_epidemic_simulator, create_sir_prior\n",
        "\n",
        "# Set style\n",
        "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
        "sns.set_palette(\"colorblind\")\n",
        "\n",
        "# Random seed\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"‚úÖ Ready to apply SBI to your problem!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Explore the Simulators\n",
        "\n",
        "Let's understand what each simulator does before choosing one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéæ Ball Throw Physics\n",
        "\n",
        "This simulator models projectile motion with air resistance:\n",
        "\n",
        "**Differential equations:**\n",
        "- Horizontal: `d¬≤x/dt¬≤ = wind - friction¬∑dx/dt`\n",
        "- Vertical: `d¬≤y/dt¬≤ = -gravity - friction¬∑dy/dt`\n",
        "\n",
        "**Parameters to infer:**\n",
        "1. Initial velocity (5-30 m/s)\n",
        "2. Launch angle (0.2-1.4 radians ‚âà 11¬∞-80¬∞)\n",
        "3. Friction coefficient (0.0-0.5)\n",
        "\n",
        "**What we observe:**\n",
        "- Landing distance (meters)\n",
        "- Maximum height reached (meters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the ball throw simulator\n",
        "test_params = torch.tensor([15.0, 0.8, 0.1])  # 15 m/s, ~45¬∞, low friction\n",
        "observations = ball_throw_simulator(test_params)\n",
        "\n",
        "print(\"üéæ Ball Throw Test:\")\n",
        "print(\n",
        "    f\"  Parameters: v‚ÇÄ={test_params[0]:.1f} m/s, Œ∏={test_params[1]:.2f} rad, Œº={test_params[2]:.2f}\"\n",
        ")\n",
        "print(\n",
        "    f\"  Observations: distance={observations[0]:.1f}m, max_height={observations[1]:.1f}m\"\n",
        ")\n",
        "\n",
        "# Visualize a trajectory\n",
        "obs, x_traj, y_traj = ball_throw_simulator(test_params, return_trajectory=True)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(x_traj, y_traj, \"b-\", linewidth=2, label=\"Trajectory\")\n",
        "plt.scatter([obs[0].item()], [0], color=\"red\", s=100, zorder=5, label=\"Landing\")\n",
        "plt.scatter(\n",
        "    [x_traj[np.argmax(y_traj)]],\n",
        "    [obs[1].item()],\n",
        "    color=\"green\",\n",
        "    s=100,\n",
        "    zorder=5,\n",
        "    label=\"Peak\",\n",
        ")\n",
        "plt.xlabel(\"Distance (m)\", fontsize=12)\n",
        "plt.ylabel(\"Height (m)\", fontsize=12)\n",
        "plt.title(\"Ball Trajectory with Air Resistance\", fontsize=14)\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(\n",
        "    \"\\nüí° We observe only the landing distance and max height, not the full trajectory!\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ü¶† SIR Epidemic Model\n",
        "\n",
        "This simulator models disease spread through a population:\n",
        "\n",
        "**Compartments:**\n",
        "- **S**usceptible: Can catch the disease\n",
        "- **I**nfected: Currently sick and contagious\n",
        "- **R**ecovered: Immune after recovery\n",
        "\n",
        "**Differential equations:**\n",
        "- `dS/dt = -Œ≤¬∑S¬∑I/N` (infection)\n",
        "- `dI/dt = Œ≤¬∑S¬∑I/N - Œ≥¬∑I` (infection - recovery)\n",
        "- `dR/dt = Œ≥¬∑I` (recovery)\n",
        "\n",
        "**Parameters to infer:**\n",
        "1. Œ≤: Infection rate (0.1-2.0 per day)\n",
        "2. Œ≥: Recovery rate (0.05-0.5 per day)\n",
        "3. I‚ÇÄ: Initial infected count (1-100 people)\n",
        "\n",
        "**What we observe:**\n",
        "- Peak number of infected\n",
        "- Time to reach peak (days)\n",
        "- Total recovered at end\n",
        "- Epidemic duration (days)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the SIR simulator\n",
        "test_params = torch.tensor([0.5, 0.1, 10])  # Œ≤=0.5, Œ≥=0.1, I‚ÇÄ=10\n",
        "observations = sir_epidemic_simulator(test_params)\n",
        "\n",
        "print(\"ü¶† SIR Epidemic Test:\")\n",
        "print(\n",
        "    f\"  Parameters: Œ≤={test_params[0]:.2f}, Œ≥={test_params[1]:.2f}, I‚ÇÄ={test_params[2]:.0f}\"\n",
        ")\n",
        "print(f\"  Basic reproduction number R‚ÇÄ = Œ≤/Œ≥ = {test_params[0] / test_params[1]:.1f}\")\n",
        "print(f\"\\n  Observations:\")\n",
        "print(f\"    Peak infected: {observations[0]:.0f} people\")\n",
        "print(f\"    Time to peak: {observations[1]:.0f} days\")\n",
        "print(f\"    Total recovered: {observations[2]:.0f} people\")\n",
        "print(f\"    Epidemic duration: {observations[3]:.0f} days\")\n",
        "\n",
        "# Visualize epidemic curves\n",
        "obs, time_series = sir_epidemic_simulator(test_params, return_time_series=True)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(\n",
        "    time_series[\"t\"], time_series[\"S\"], label=\"Susceptible\", linewidth=2, color=\"blue\"\n",
        ")\n",
        "plt.plot(time_series[\"t\"], time_series[\"I\"], label=\"Infected\", linewidth=2, color=\"red\")\n",
        "plt.plot(\n",
        "    time_series[\"t\"], time_series[\"R\"], label=\"Recovered\", linewidth=2, color=\"green\"\n",
        ")\n",
        "\n",
        "# Mark observations\n",
        "peak_idx = np.argmax(time_series[\"I\"])\n",
        "plt.scatter(\n",
        "    [time_series[\"t\"][peak_idx]],\n",
        "    [time_series[\"I\"][peak_idx]],\n",
        "    color=\"red\",\n",
        "    s=100,\n",
        "    zorder=5,\n",
        "    label=f\"Peak: {obs[0]:.0f}\",\n",
        ")\n",
        "\n",
        "plt.xlabel(\"Time (days)\", fontsize=12)\n",
        "plt.ylabel(\"Number of individuals\", fontsize=12)\n",
        "plt.title(\"SIR Epidemic Dynamics (Population = 10,000)\", fontsize=14)\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüí° We observe summary statistics, not the full time series!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üî¨ Your Own Simulator\n",
        "\n",
        "If you brought your own simulator, adapt this template:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def your_simulator(params):\n",
        "    \"\"\"\n",
        "    Template for your own simulator.\n",
        "\n",
        "    Requirements:\n",
        "    1. Takes parameters (torch.Tensor or numpy array)\n",
        "    2. Returns observations (torch.Tensor)\n",
        "    3. Should include some stochasticity (noise)\n",
        "    4. Runs reasonably fast (< 1 second)\n",
        "    \"\"\"\n",
        "    # Convert to torch if needed\n",
        "    if isinstance(params, np.ndarray):\n",
        "        params = torch.tensor(params, dtype=torch.float32)\n",
        "\n",
        "    # Your simulation code here\n",
        "    # ...\n",
        "\n",
        "    # Add observation noise (important!)\n",
        "    # observations = observations * (1 + torch.randn_like(observations) * 0.05)\n",
        "\n",
        "    # Return as torch tensor\n",
        "    # return torch.tensor(observations, dtype=torch.float32)\n",
        "\n",
        "    pass  # Remove this when implementing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Choose Your Simulator and Run SBI\n",
        "\n",
        "**üëá Choose ONE option below by uncommenting the appropriate section:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== OPTION A: Ball Throw ==========\n",
        "simulator = ball_throw_simulator\n",
        "prior = create_ball_throw_prior(include_wind=False)  # Set True to include wind\n",
        "param_names = [\"v‚ÇÄ (m/s)\", \"Œ∏ (rad)\", \"Œº (friction)\"]\n",
        "obs_names = [\"distance (m)\", \"max height (m)\"]\n",
        "\n",
        "# ========== OPTION B: SIR Model ==========\n",
        "# simulator = sir_epidemic_simulator\n",
        "# prior = create_sir_prior()\n",
        "# param_names = [\"Œ≤ (infection)\", \"Œ≥ (recovery)\", \"I‚ÇÄ (initial)\"]\n",
        "# obs_names = [\"peak infected\", \"time to peak\", \"total recovered\", \"duration\"]\n",
        "\n",
        "# ========== OPTION C: Your Simulator ==========\n",
        "# simulator = your_simulator\n",
        "# prior = utils.BoxUniform(\n",
        "#     low=torch.tensor([...]),   # Your parameter lower bounds\n",
        "#     high=torch.tensor([...])   # Your parameter upper bounds\n",
        "# )\n",
        "# param_names = [...]  # Your parameter names\n",
        "# obs_names = [...]    # Your observable names\n",
        "\n",
        "print(f\"Selected simulator: {simulator.__name__}\")\n",
        "print(f\"Parameters: {param_names}\")\n",
        "print(f\"Observables: {obs_names}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Generate \"Observed\" Data\n",
        "\n",
        "In a real application, this would be your experimental data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate synthetic observation (ground truth for testing)\n",
        "true_params = prior.sample()\n",
        "observed_data = simulator(true_params)\n",
        "\n",
        "print(\"\\nüéØ True parameters (hidden in real applications):\")\n",
        "for i, name in enumerate(param_names):\n",
        "    print(f\"  {name}: {true_params[i]:.3f}\")\n",
        "\n",
        "print(\"\\nüìä Observed data:\")\n",
        "for i, name in enumerate(obs_names):\n",
        "    print(f\"  {name}: {observed_data[i]:.3f}\")\n",
        "\n",
        "print(\"\\nüé≤ Challenge: Can we recover the true parameters from observations alone?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Run Neural Posterior Estimation üöÄ\n",
        "\n",
        "The same 4-step workflow from Exercise 1!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Create NPE object\n",
        "npe = inference.NPE(prior=prior)\n",
        "\n",
        "# Step 2: Train on simulations\n",
        "print(\"üèÉ Training neural network...\")\n",
        "print(\"This will take 20-40 seconds...\\n\")\n",
        "\n",
        "npe = npe.append_simulations(\n",
        "    simulator,\n",
        "    num_simulations=5000,  # Use 10000+ for better results\n",
        ").train()\n",
        "\n",
        "# Step 3: Build posterior for our observation\n",
        "posterior = npe.build_posterior()\n",
        "\n",
        "# Step 4: Sample from posterior\n",
        "posterior_samples = posterior.sample((2000,), x=observed_data, show_progress_bars=True)\n",
        "\n",
        "print(\"\\n‚úÖ Inference complete! Let's see what we learned...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Visualize Results üìä"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare posterior to prior and truth\n",
        "fig, axes = plt.subplots(1, len(true_params), figsize=(4 * len(true_params), 4))\n",
        "\n",
        "# Handle single parameter case\n",
        "if len(true_params) == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for i, ax in enumerate(axes):\n",
        "    # Posterior\n",
        "    ax.hist(\n",
        "        posterior_samples[:, i],\n",
        "        bins=30,\n",
        "        alpha=0.7,\n",
        "        density=True,\n",
        "        label=\"Posterior\",\n",
        "        color=\"green\",\n",
        "        edgecolor=\"darkgreen\",\n",
        "    )\n",
        "\n",
        "    # Prior\n",
        "    prior_samples = prior.sample((1000,))\n",
        "    ax.hist(\n",
        "        prior_samples[:, i],\n",
        "        bins=30,\n",
        "        alpha=0.3,\n",
        "        density=True,\n",
        "        label=\"Prior\",\n",
        "        color=\"gray\",\n",
        "        edgecolor=\"black\",\n",
        "    )\n",
        "\n",
        "    # Truth\n",
        "    ax.axvline(\n",
        "        true_params[i], color=\"red\", linewidth=2, linestyle=\"--\", label=\"True value\"\n",
        "    )\n",
        "\n",
        "    # Posterior mean\n",
        "    post_mean = posterior_samples[:, i].mean()\n",
        "    ax.axvline(post_mean, color=\"blue\", linewidth=2, linestyle=\"-\", label=\"Post. mean\")\n",
        "\n",
        "    ax.set_xlabel(param_names[i], fontsize=12)\n",
        "    ax.set_ylabel(\"Density\", fontsize=12)\n",
        "    ax.legend(fontsize=10, loc=\"upper right\")\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle(\"Posterior vs Prior: Learning from Data\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print quantitative summary\n",
        "print(\"\\nüìà Parameter Recovery Summary:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"{'Parameter':<20} {'True':<10} {'Post. Mean ¬± Std':<20} {'Error':<10}\")\n",
        "print(\"-\" * 60)\n",
        "for i, name in enumerate(param_names):\n",
        "    true_val = true_params[i].item()\n",
        "    post_mean = posterior_samples[:, i].mean().item()\n",
        "    post_std = posterior_samples[:, i].std().item()\n",
        "    error = abs(true_val - post_mean)\n",
        "    print(\n",
        "        f\"{name:<20} {true_val:<10.3f} {post_mean:.3f} ¬± {post_std:.3f}  {error:<10.3f}\"\n",
        "    )\n",
        "\n",
        "# Calculate credible intervals\n",
        "print(\"\\nüìä 95% Credible Intervals:\")\n",
        "for i, name in enumerate(param_names):\n",
        "    q_low = torch.quantile(posterior_samples[:, i], 0.025).item()\n",
        "    q_high = torch.quantile(posterior_samples[:, i], 0.975).item()\n",
        "    true_val = true_params[i].item()\n",
        "    in_ci = \"‚úÖ\" if q_low <= true_val <= q_high else \"‚ùå\"\n",
        "    print(f\"  {name}: [{q_low:.3f}, {q_high:.3f}] {in_ci}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Diagnostic - Posterior Predictive Check üîç\n",
        "\n",
        "Can parameters from our posterior reproduce the observed data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate posterior predictive samples\n",
        "n_predictive = 200\n",
        "predictive_data = []\n",
        "\n",
        "print(\"Generating posterior predictive samples...\")\n",
        "for _ in range(n_predictive):\n",
        "    # Sample parameters from posterior\n",
        "    param_sample = posterior.sample((1,), x=observed_data)\n",
        "    # Simulate with those parameters\n",
        "    sim_data = simulator(param_sample[0])\n",
        "    predictive_data.append(sim_data)\n",
        "\n",
        "predictive_data = torch.stack(predictive_data)\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, len(observed_data), figsize=(4 * len(observed_data), 4))\n",
        "\n",
        "if len(observed_data) == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for i, ax in enumerate(axes):\n",
        "    # Predictive distribution\n",
        "    ax.hist(\n",
        "        predictive_data[:, i],\n",
        "        bins=25,\n",
        "        alpha=0.6,\n",
        "        label=\"Predictive\",\n",
        "        color=\"blue\",\n",
        "        density=True,\n",
        "    )\n",
        "\n",
        "    # Observed value\n",
        "    ax.axvline(\n",
        "        observed_data[i], color=\"red\", linewidth=2, linestyle=\"--\", label=\"Observed\"\n",
        "    )\n",
        "\n",
        "    # Add percentiles\n",
        "    q5 = torch.quantile(predictive_data[:, i], 0.05)\n",
        "    q95 = torch.quantile(predictive_data[:, i], 0.95)\n",
        "    ax.axvspan(q5, q95, alpha=0.2, color=\"blue\", label=\"90% CI\")\n",
        "\n",
        "    ax.set_xlabel(obs_names[i], fontsize=12)\n",
        "    ax.set_ylabel(\"Density\", fontsize=12)\n",
        "    ax.legend(fontsize=10)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle(\"Posterior Predictive Check: Can We Reproduce the Data?\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Check if observations fall within predictive distribution\n",
        "print(\"\\n‚úÖ Diagnostic Results:\")\n",
        "for i, name in enumerate(obs_names):\n",
        "    percentile = (predictive_data[:, i] < observed_data[i]).float().mean() * 100\n",
        "    print(f\"  {name}: Observed value is at {percentile:.1f}th percentile\")\n",
        "    if 5 < percentile < 95:\n",
        "        print(f\"    ‚úÖ Well within predictive distribution\")\n",
        "    else:\n",
        "        print(f\"    ‚ö†Ô∏è Near edge of predictive distribution\")\n",
        "\n",
        "print(\"\\nüí° If observed data falls within the predictive distribution,\")\n",
        "print(\"   our posterior is consistent with the observations!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: Explore Parameter Correlations üîó"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(param_names) > 1:\n",
        "    # Compute correlation matrix\n",
        "    correlation_matrix = torch.corrcoef(posterior_samples.T)\n",
        "\n",
        "    # Visualize\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    im = plt.imshow(correlation_matrix, cmap=\"RdBu_r\", vmin=-1, vmax=1)\n",
        "    plt.colorbar(im, label=\"Correlation\")\n",
        "\n",
        "    # Add labels\n",
        "    n_params = len(param_names)\n",
        "    plt.xticks(range(n_params), param_names, rotation=45, ha=\"right\")\n",
        "    plt.yticks(range(n_params), param_names)\n",
        "\n",
        "    # Add correlation values\n",
        "    for i in range(n_params):\n",
        "        for j in range(n_params):\n",
        "            text = plt.text(\n",
        "                j,\n",
        "                i,\n",
        "                f\"{correlation_matrix[i, j]:.2f}\",\n",
        "                ha=\"center\",\n",
        "                va=\"center\",\n",
        "                color=\"white\" if abs(correlation_matrix[i, j]) > 0.5 else \"black\",\n",
        "                fontsize=12,\n",
        "                fontweight=\"bold\",\n",
        "            )\n",
        "\n",
        "    plt.title(\"Parameter Correlations in Posterior\", fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Identify strong correlations\n",
        "    print(\"\\nüîó Parameter Correlations:\")\n",
        "    for i in range(n_params):\n",
        "        for j in range(i + 1, n_params):\n",
        "            corr = correlation_matrix[i, j].item()\n",
        "            if abs(corr) > 0.3:\n",
        "                strength = \"Strong\" if abs(corr) > 0.7 else \"Moderate\"\n",
        "                direction = \"positive\" if corr > 0 else \"negative\"\n",
        "                print(\n",
        "                    f\"  {param_names[i]} ‚Üî {param_names[j]}: {strength} {direction} ({corr:.2f})\"\n",
        "                )\n",
        "\n",
        "    print(\"\\nüí° Correlations reveal parameter trade-offs and identifiability issues!\")\n",
        "else:\n",
        "    print(\"Single parameter - no correlations to show.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 8: Experiments & What-If Analysis üî¨\n",
        "\n",
        "Let's explore how different choices affect our inference!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment 1: What if we had noisier observations?\n",
        "print(\"üî¨ Experiment 1: Effect of Observation Noise\\n\")\n",
        "\n",
        "# Add extra noise to observations\n",
        "noisy_obs = observed_data * (1 + torch.randn_like(observed_data) * 0.2)\n",
        "\n",
        "# Get posterior for noisy data\n",
        "noisy_posterior_samples = posterior.sample((500,), x=noisy_obs)\n",
        "\n",
        "print(f\"Original observations: {observed_data.numpy()}\")\n",
        "print(f\"Noisy observations:    {noisy_obs.numpy()}\")\n",
        "print(f\"\\nPosterior uncertainty (std):\")\n",
        "print(f\"  Original: {posterior_samples.std(dim=0).numpy()}\")\n",
        "print(f\"  Noisy:    {noisy_posterior_samples.std(dim=0).numpy()}\")\n",
        "print(\"\\nüí° Noisier observations ‚Üí higher posterior uncertainty!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment 2: How many simulations do we really need?\n",
        "print(\"üî¨ Experiment 2: Effect of Training Data Size\\n\")\n",
        "\n",
        "# Train with fewer simulations\n",
        "small_npe = inference.NPE(prior=prior)\n",
        "small_npe = small_npe.append_simulations(\n",
        "    simulator,\n",
        "    num_simulations=500,  # 10x fewer!\n",
        ").train(show_train_summary=False)\n",
        "\n",
        "small_posterior = small_npe.build_posterior()\n",
        "small_samples = small_posterior.sample((500,), x=observed_data)\n",
        "\n",
        "# Compare accuracy\n",
        "print(\"Parameter recovery (distance from truth):\")\n",
        "for i, name in enumerate(param_names):\n",
        "    error_5000 = abs(posterior_samples[:, i].mean() - true_params[i]).item()\n",
        "    error_500 = abs(small_samples[:, i].mean() - true_params[i]).item()\n",
        "    print(f\"  {name}:\")\n",
        "    print(f\"    5000 sims: error = {error_5000:.4f}\")\n",
        "    print(f\"    500 sims:  error = {error_500:.4f}\")\n",
        "\n",
        "print(\"\\nüí° More simulations ‚Üí better parameter recovery!\")\n",
        "print(\"   But diminishing returns after ~10,000 simulations.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Challenge: Multiple Observations\n",
        "\n",
        "What if you had multiple independent observations?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate multiple observations with the same true parameters\n",
        "n_observations = 3\n",
        "multi_obs = torch.stack([simulator(true_params) for _ in range(n_observations)])\n",
        "\n",
        "print(f\"üîÑ Generated {n_observations} independent observations:\")\n",
        "for i, obs in enumerate(multi_obs):\n",
        "    print(f\"  Obs {i + 1}: {obs.numpy()}\")\n",
        "\n",
        "# Strategy 1: Use the mean observation\n",
        "mean_obs = multi_obs.mean(dim=0)\n",
        "print(f\"\\nüìä Mean observation: {mean_obs.numpy()}\")\n",
        "\n",
        "# Get posterior for mean observation\n",
        "multi_posterior_samples = posterior.sample((1000,), x=mean_obs)\n",
        "\n",
        "# Compare uncertainties\n",
        "print(\"\\nüìà Posterior uncertainty (std):\")\n",
        "print(f\"  Single obs:   {posterior_samples.std(dim=0).numpy()}\")\n",
        "print(f\"  Mean of {n_observations} obs: {multi_posterior_samples.std(dim=0).numpy()}\")\n",
        "\n",
        "print(\"\\n‚úÖ Multiple observations reduce uncertainty!\")\n",
        "print(\"\\nüí° Advanced: For proper treatment of multiple observations,\")\n",
        "print(\"   retrain NPE with concatenated observations or use Sequential NPE.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéâ Congratulations!\n",
        "\n",
        "You've successfully:\n",
        "- ‚úÖ Applied SBI to different problems\n",
        "- ‚úÖ Learned the universal NPE workflow\n",
        "- ‚úÖ Performed diagnostic checks\n",
        "- ‚úÖ Explored how choices affect inference\n",
        "\n",
        "### üîë Key Takeaways:\n",
        "\n",
        "1. **SBI is universal** - Same workflow for any simulator!\n",
        "2. **Prior choice matters** - Must cover true parameters\n",
        "3. **Diagnostics are essential** - Always check predictive distributions\n",
        "4. **More data = less uncertainty** - Both simulations and observations help\n",
        "5. **Parameters can be correlated** - Trade-offs and identifiability\n",
        "\n",
        "### üöÄ Next Steps:\n",
        "\n",
        "For your research:\n",
        "1. **Start simple** - Test with synthetic data first\n",
        "2. **Scale up gradually** - Increase complexity step by step\n",
        "3. **Use Sequential NPE** - More efficient for expensive simulators\n",
        "4. **Try other methods** - NLE, NRE for different use cases\n",
        "\n",
        "### üìö Resources:\n",
        "\n",
        "- üìñ [SBI Documentation](https://sbi-dev.github.io/sbi)\n",
        "- üíª [GitHub Repository](https://github.com/sbi-dev/sbi)\n",
        "- üì∞ [JOSS Paper](https://joss.theoj.org/papers/10.21105/joss.02505)\n",
        "- üí¨ [Community Forum](https://github.com/sbi-dev/sbi/discussions)\n",
        "\n",
        "---\n",
        "\n",
        "## üôè Thank you for participating!\n",
        "\n",
        "**Now go forth and quantify uncertainty in your simulators!**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "euroscipy-2025-sbi-tutorial (3.12.9)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
